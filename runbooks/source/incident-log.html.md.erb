---
title: Incident Log
weight: 45
---

# Incident Log

## September 2020

### Incident on 2020-09-07 12.54 UTC - All users are unable to create new ingress rules.

- **Status**: Resolved at 2020-09-07 15.56 UTC

- **Incident**: The Ingress API refused 100% of POST requests.

- **Impact**:
    - If a user were to provision a new service, they would be unable to create an ingress into the cluster.

- **Context**:
  * [Version 0.1.0](https://github.com/ministryofjustice/cloud-platform-terraform-teams-ingress-controller/compare/0.0.9...0.1.0) of the [teams ingress controller module](https://github.com/ministryofjustice/cloud-platform-terraform-teams-ingress-controller) enabled the creation of a `validationwebhookconfiguration` resource.
  * By enabling this option we created a single point of failure for all ingress-controller pods in the `ingress-controller` namespace.
  * A new 0.1.0 ingress controller failed to create in the "live-1" cluster due to AWS resource limits.
  * Validation webhook stopped new rules from creating, with the error:
  ```
  Error from server (InternalError): error when creating "ingress.yaml": Internal error occurred: failed calling webhook "validate.nginx.ingress.kubernetes.io": Post https://offender-categorisation-prod-nx-controller-admission.ingress-controllers.svc:443/extensions/v1beta1/ingresses?timeout=30s: x509: certificate signed by unknown authority
  ```
  * Initial investigation thread: https://mojdt.slack.com/archives/C514ETYJX/p1599478794246900
  * Incident declared: https://mojdt.slack.com/archives/C514ETYJX/p1599479640251900

- **Resolution**:
    The team manually removed the all the additional admission controllers created by 0.1.0. They then removed the admission webhook from the module and created a new release (0.1.1). All ingress modules currently on 0.1.0 were upgraded to the new release 0.1.1.

## August 2020

### Incident on 2020-08-25 11.26 UTC - Connectivity issues with eu-west-2a

- **Status**: Resolved at 2020-08-25 12.30 UTC

- **Incident**: The AWS  Availability Zones `eu-west-2a`, which contain some of our kubernetes nodes had an outage. API latency was elevated, some EC2 became unreachable and overall connectivity was unstable.

- **Impact**:  
    - Two kubernetes nodes became unreachable
    - No new node could be launched in eu-west-2a 
    - Kubernetes had issues talking to some of these nodes, preventing some API calls to succeed (Pods were not terminating)
    - New pods were not able to pull their Docker images.

- **Context**:
    * Pods and Nodes sitting in other Availability Zones (b & c) were not impacted
    * Slack threads: [Issue detected](https://mojdt.slack.com/archives/C514ETYJX/p1598351210195200), [Incident Declared](https://mojdt.slack.com/archives/C514ETYJX/p1598351210195200),
    * We now have 25 pods in the cluster, instead of 21

- **Resolution**:
    The incident was mitigated by deploying more 2-4 nodes in healthy Availability Zones, manually deleting the non-responding pods ,and terminating the impacted nodes

### Incident on 2020-08-14  10.43 UTC - Ingress-controllers crashlooping

- **Status**: Resolved at 2020-08-14 11:38 UTC

- **Incident**: There are 6 replicas of the ingress-controller pod and 2 out of the 6 were crashlooping. A restart of the pods did not resolve the issue. As per a normal runbook process, a recycle of all pods was required. However after restarting pods 4 and 5, they also started to crashloop. The risk was when restarting pods 5 and 6 -  all 6 pods could be down and all ingresses down for the cluster.

- **Impact**:
    - Increased risk for all ingresses failing in the cluster if all 6 ingress-controller pods are in a crashloop state.

- **Context**:
    * 2 of the 6 ingress-controller pods crashlooping, after restart of 4 pods, 4 out of 6 pods crashlooping.
    * Issue was with the leader ingress-controller pod (which was not identified or restarted yet) and exhausting the shared memory.
    * After a restart of the leader ingress-controller pod, all other pods reverted back to a ready/running state.
    * Timeline : [https://docs.google.com/document/d/1kxKwC1B_pnlPbysS0zotbXMKyZcUDmDtnGbEyIHGvgQ/edit#heading=h.z3py6eydx4qu](https://docs.google.com/document/d/1kxKwC1B_pnlPbysS0zotbXMKyZcUDmDtnGbEyIHGvgQ/edit#heading=h.z3py6eydx4qu)
    * Slack thread: [https://mojdt.slack.com/archives/C514ETYJX/p1597399295031000](https://mojdt.slack.com/archives/C514ETYJX/p1597399295031000),

- **Resolution**:
    A restart of the leader ingress-controller pod was required so the other pods in the replica-set could connect and get the latest nginx.config file.

### Incident on 2020-08-07 16:39 UTC - Master node provisioning failure

- **Status**: Resolved at 2020-08-14 10:06 UTC

- **Incident**: Routine replacement of a master node failed because AWS did not have any c4.4xlarge instances available in the relevant availability zone.

- **Impact**:
    - Increased risk because the cluster was running on 2 out of 3 master nodes, for a brief period

- **Context**:
    * Lack of availability of a given instance type is not a failure mode for which we have planned
    * In theory, if a problem occurs which eventually kills each master node in turn, and if instances of the right type are not available in at least 2 availability zones, this could bring down the whole cluster.
    * Timeline : [https://docs.google.com/document/d/1SOAOeL-89cuK-_fJbtgYcArInWQY7UXiIDY7wN5gjuA/edit#](https://docs.google.com/document/d/1SOAOeL-89cuK-_fJbtgYcArInWQY7UXiIDY7wN5gjuA/edit#)
ttps://docs.google.com/document/d/1kxKwC1B_pnlPbysS0zotbXMKyZcUDmDtnGbEyIHGvgQ/edit#heading=h.z3py6eydx4qu)
    * Slack thread: [https://mojdt.slack.com/archives/C514ETYJX/p1596814746202600](https://mojdt.slack.com/archives/C514ETYJX/p1596814746202600)

- **Resolution**:
    * A new c4.4xlarge node *was* successfully (and automatically) launched approx. 40 minutes after we saw the problem
    * We replaced all our master nodes with c5.4xlarge instances, which (currently) have better availability
    * We and AWS are still investigating longer-term and more reliable fixes

### Incident on 2020-08-04  17:13 UTC

- **Status**: Resolved at 2020-08-05 16:16 UTC

- **Incident**: Integration tests failed for cert-manager, apply pipeline failed showing it doesnot have permissions and
 divergence pipeline shows drift for live-1 components

- **Impact**:
    - Increased risk for cluster failure because some of the components doesnot have the correct configuration needed for the `live-1` production cluster

- **Context**:
    * One of the engineers was creating a test EKS cluster and ran `terraform apply` on EKS components
    * Without fully aware of the current cluster context, the `terraform apply` for EKS test cluster components has been applied to the `live-1` kops cluster
    * This has changed the configuration of several resources in the `live-1` cluster
    * Timeline : [https://docs.google.com/document/d/1VrABxeHLMOnoM4yYoCi9N4N4zRY1SK1hTjrZ9s05zuc/edit?usp=sharing](https://docs.google.com/document/d/1VrABxeHLMOnoM4yYoCi9N4N4zRY1SK1hTjrZ9s05zuc/edit?usp=sharing)
    * Slack thread: [https://mojdt.slack.com/archives/C514ETYJX/p1596621864015400](https://mojdt.slack.com/archives/C514ETYJX/p1596621864015400),

- **Resolution**:
    Compare each resource configuration with the terraform state and applied the correct configuration from the code specific to kops cluster

### Incident on 2020-08-04  17:13 UTC

- **Status**: Resolved at 2020-08-05 16:16 UTC

- **Incident**: Integration tests failed for cert-manager, apply pipeline failed showing it doesnot have permissions and
 divergence pipeline shows drift for live-1 components

- **Impact**:
    - Increased risk for cluster failure because some of the components doesnot have the correct configuration needed for the `live-1` production cluster

- **Context**:
    * One of the engineers was creating a test EKS cluster and ran `terraform apply` on EKS components
    * Without fully aware of the current cluster context, the `terraform apply` for EKS test cluster components has been applied to the `live-1` kops cluster
    * This has changed the configuration of several resources in the `live-1` cluster
    * Timeline : [https://docs.google.com/document/d/1VrABxeHLMOnoM4yYoCi9N4N4zRY1SK1hTjrZ9s05zuc/edit?usp=sharing](https://docs.google.com/document/d/1VrABxeHLMOnoM4yYoCi9N4N4zRY1SK1hTjrZ9s05zuc/edit?usp=sharing)
    * Slack thread: [https://mojdt.slack.com/archives/C514ETYJX/p1596621864015400](https://mojdt.slack.com/archives/C514ETYJX/p1596621864015400),

- **Resolution**:
    Compare each resource configuration with the terraform state and applied the correct configuration from the code specific to kops cluster

## April 2020

### Incident on 2020-04-15 10:58 UTC

- **Status**: Resolved at 2020-04-15 15:09 UTC

- **Incident**: After an upgrade of the Nginx ingresses, support for legacy TLS was dropped.

- **Impact**:
    - IE11 users could not access any services running on the Cloud Platform
    - A few teams came forward with the issue :
    --- LAA
    --- Correspondence Tool
    --- Prisoner Money

- **Context**:
    * After an upgrade of the Nginx Helm chart v1.24.0 to v1.35
    * The current version of Nginx has deprecated support for TLS < 1.3
    * The issue was spotted on IE11 browsers.
    * Timeline : [https://docs.google.com/document/d/1SCf1WT82IlBYWozWN_FXZqL5h0KUcul_QAkxE84YDw0/edit?usp=sharing](https://docs.google.com/document/d/1SCf1WT82IlBYWozWN_FXZqL5h0KUcul_QAkxE84YDw0/edit?usp=sharing)
    * Slack thread: [https://mojdt.slack.com/archives/C57UPMZLY/p1586954463298700](https://mojdt.slack.com/archives/C57UPMZLY/p1586954463298700)

- **Resolution**:
    The Nginx configuration was modified to enable TLSv1, TLSv1.1 and TLSv1.2


## February 2020

### Incident on 2020-02-25 10:58 UTC

- **Status**: Resolved at 2020-02-25 17:07 UTC

- **Incident**: During an upgrade, new masters were not coming up correctly (missing calico networking and other pods)

- **Impact**:
    - Degraded kubernetes API performance (because some API calls were being directed to non-functioning masters)
    - Increased risk of cluster failure, because we were running on a single master during the incident

- **Context**:
    * Upgrading from kubernetes 1.13.12 to 1.14.10, kops 1.13.2 to 1.14.1
    * The first master was replaced fine, but the second didn't have calico and some other essential pods, and was not functioning correctly
    * Attempting to roll back the upgrade, every new master exhibited the same problem
    * Slack thread: [https://mojdt.slack.com/archives/C514ETYJX/p1582628309085600](https://mojdt.slack.com/archives/C514ETYJX/p1582628309085600)

- **Resolution**:
    The `kube-system` namespace has a label, `openpolicyagent.org/webhook: ignore` This label tells the Open Policy Agent (OPA) that pods are allowed to run in this namespace on the master nodes. Somehow, this label got removed, so the OPA was preventing pods from running on the new master nodes, as each one came up, so the new master was unable to launch essential pods such as `calico` and `fluentd`.

### Incident on 2020-02-18 14:13 UTC

- **Status**: Resolved at 2020-02-18 14:59 UTC

- **Incident**: Pingdom reported that Prometheus was down (prometheus.cloud-platform.service.justice.gov.uk).

- **Impact**:
    - The prometheus dashboard was unavailable for everyone, for the whole duration of the incident.
    - Between 2020-02-18 14:22 and 2020-02-18 14:26, prometheus could not receive metrics.

- **Context**:
   - Although the Prometheus URL was unreachable, Grafana and Alertmanager were resolving.
   - There seemed to be an issue preventing requests to reach the prometheus pods.
   - Disk space and other resources, the usual suspects, were ruled out as the cause.
   - The domain name amd ingress were both valid.

- **Resolution**:
    We suspect an intermittent & external networking issue to be the cause of this outage.


### Incident on 2020-02-12 11:45 UTC

- **Status**: Resolved at 2020-02-18 12:07 UTC

- **Identified**: Pingdom reported Concourse (concourse.cloud-platform.service.justice.gov.uk) down.

- **Context**: One of the engineers was deleting old clusters (he ran `terraform destroy`) and he wasn't fully aware in which _terraform workspace_ was working on. Using `terraform destroy`, EKS nodes/workers were deleted from the manager cluster.

- **Resolution**: Using terraform (`terraform apply -var-file vars/manager.tfvars` specifically) the cluster nodes where created and the infrastructure aligned? to the desired terraform state


