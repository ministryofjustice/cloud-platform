---
title: Running Kops Update and Rollingupdate
weight: 700
---

# Running Kops Update and Rollingupdate

We want to ensure Kops is updated regularly so there is a task in every sprint to run Kops update. It essentially updates various components / configuration coming from kops.

Note: This is for a kops update not an upgrade. 

## Pre-requisites

Before you begin, there are a few pre-reqs:

- You are on the right cluster to run the Kops update.

```bash
$ kubectl config current-context
```

- Your are on the right AWS_PROFILE where the k8s cluster is created.

```bash
$ export AWS_PROFILE=xxxx
```

- Set the --state flag or export KOPS_STATE_STORE

```bash
$ export KOPS_STATE_STORE=s3://<bucket>
```

for live-1 KOPS_STATE_STORE=s3://cloud-platform-kops-state

## Running Kops update

Run kops update cluster Without `--yes` to check the configuration changes for the cluster, it will show you a preview of what it is going to do, this is handy to check what the changes are before applying the changes. Sometimes you will also have to kops rolling-update cluster to roll out the configuration immediately.

```bash
kops update cluster ${CLUSTER_NAME}.cloud-platform.service.justice.gov.uk
```

Once you are happy, kops update cluster `--yes` to apply the changes.

```bash
kops update cluster ${CLUSTER_NAME}.cloud-platform.service.justice.gov.uk --yes
```

It should report

> 
Cluster changes have been applied to the cloud.

##  Running Kops rolling-update

This command updates a kubernetes cluster to match the cloud and kops specifications.

To perform a rolling update, you need to update the cloud resources first with the command kops update cluster.

If rolling-update does not report that the cluster needs to be rolled, you can force the cluster to be rolled with the force flag. 

Rolling update drains and validates the cluster by default. A cluster is deemed validated when all required nodes are running and all pods in the kube-system namespace are operational. When a node is deleted, rolling-update sleeps the interval for the node type, and then tries for the same period of time for the cluster to be validated. 

For instance, setting --master-interval=4m causes rolling-update to wait for 4 minutes after a master is rolled, and another 4 minutes for the cluster to stabilize and pass validation.

```bash
kops rolling-update cluster ${CLUSTER_NAME}.cloud-platform.service.justice.gov.uk --yes
```

It should report below steps for all the nodes in the cluster.

```
Draining the node: “${node}”.
node/${node} cordoned
WARNING: Deleting pods with local storage: ${pod names}
pod/${pod1 name} evicted
pod/${pod2 name} evicted
…
pod/${podn name} evicted
Waiting for 1m30s for pods to stabilize after draining.
deleting node “${node}” from kubernetes
Stopping instance “${aws-instance}”, node  “${node}”, in group "nodes.${CLUSTER_NAME}.cloud-platform.service.justice.gov.uk" (this may take a while).
waiting for 4m0s after terminating instance
Validating the cluster.
Cluster did not pass validation, will try again in "30s" until duration "5m0s" expires: machine “${aws-instance}” has not yet joined cluster.
Cluster validated.
```

Open new tab and watch the status of the rolling-update

```bash
watch -n10 kubectl get nodes --sort-by .metadata.creationTimestamp
```

Once the rolling-update is completed it should report

> Rolling update completed for cluster ${CLUSTER_NAME}!

Check the cluster status with:

```bash
$ kops validate cluster
```

It should report

> Your cluster `${CLUSTER_NAME}.cloud-platform.service.justice.gov.uk is ready`

## Possible Errors

While draining a node there is a chance that rolling update might be stuck at evicting a pod, then get the node it is stuck on and identify the pods running on there and delete the pod manually.

```bash
$ kubectl get pods -owide --all-namespaces | grep ${node_name}
```
