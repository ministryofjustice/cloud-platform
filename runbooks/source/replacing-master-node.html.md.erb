---
title: Replacing a master node
weight: 225
last_reviewed_on: 2020-05-06
review_in: 3 months
---

# Replacing a master node

If one of the master nodes fails due to hardware failure, data directory corruption, etcd volume deletion, or some other problem, it should be replaced as soon as possible.

To replace the Master node, follow the instructions below for removing the unhealthy member from the cluster, and then re-adding the master node and its volumes via kops


## Pre-requisites

Before you begin, there are a few pre-reqs:

- You are targeting the right cluster.

```bash
$ kubectl config current-context
```

Expected outcome example: `live-1.cloud-platform.service.justice.gov.uk`

- You are using the right AWS_PROFILE for this cluster.

```bash
$ export AWS_PROFILE=moj-cp
```

- Export KOPS_STATE_STORE

```bash
$ export KOPS_STATE_STORE=s3://cloud-platform-kops-state
```

## Identify the failed master node

Run kops validate cluster to identify failed master node.

```bash
kops validate cluster ${CLUSTER_NAME}.cloud-platform.service.justice.gov.uk
```

When a master node has failed, you will see validation errors like this:

```bash
VALIDATION ERRORS
KIND    NAME            MESSAGE
Machine i-046ed8d94d65719a6 machine "i-046ed8d94d65719a6" has not yet joined cluster
```

This shows that one of the machines not joined the cluster. 

Make sure it is a master node by looking for the Instance ID in the Amazon EC2 console, on the Instances page. The instance ID should be something like: 

```
master-<availability-zone>.masters.<cluster_name>.cloud-platform.service.justice.gov.uk
``` 
 
##  Remove the unhealthy master node.

As per [this guidance][remove-member-first] remove the unhealthy etcd member (i.e. the failed master node) first. 
 
Identify and remove the unhealthy member by accessing any of the working master nodes using SSH, via the bastion instance.

In the Amazon EC2 console, on the Instances page, search using `${CLUSTER_NAME}`to locate the bastion instance:
 
```
 bastion.<cluster_name>.cloud-platform.service.justice.gov.uk
 ```
 
Use the Public DNS (IPv4) in the description of the Instance to login in to bastion like this:


```bash
ssh -A admin@ec2-35-176-xx-xx.eu-west-2.compute.amazonaws.com -p 50422
```

From the bastion, login to any of the working master nodes:

```bash
ssh 172.20.xx.xx
```

While SSHed onto a master:

```bash
$ sudo docker ps | grep etcd.manager
```

You will see output like this:

```
e81b4622417b  kopeio/etcd-manager         k8s_etcd-manager_etcd-manager-main-...
39c186fba5ea  kopeio/etcd-manager         k8s_etcd-manager_etcd-manager-events-...
8c77d710ce46  k8s.gcr.io/pause-amd64:3.0  k8s_POD_etcd-manager-main-...
806e5af8125d  k8s.gcr.io/pause-amd64:3.0  k8s_POD_etcd-manager-events-...
```

Pick the kopeio/etcd-manager container you're interested in ('main' or 'events') and then docker exec onto it

```bash
$ sudo docker exec -it [container id] bash
```

Run etcdctl like this to get the member list of `etcd-main`. Change the `PORT=4002` and `ETCD=etcd-events` and run it again to get the member list of `etcd-events`

```bash
cd /opt/etcd-v3.3.10-linux-amd64/
CLUSTER=live-1
PORT=4001
ETCD=etcd
ETCDCTL_API=3 \
  ./etcdctl --key /rootfs/etc/kubernetes/pki/kube-apiserver/etcd-client.key \
  --cert  /rootfs/etc/kubernetes/pki/kube-apiserver/etcd-client.crt \
  --cacert /rootfs/etc/kubernetes/pki/kube-apiserver/etcd-ca.crt  \
  --endpoints "https://${ETCD}-a.internal.${CLUSTER}.cloud-platform.service.justice.gov.uk:${PORT},https://${ETCD}-b.internal.${CLUSTER}.cloud-platform.service.justice.gov.uk:${PORT},https://${ETCD}-c.internal.${CLUSTER}.cloud-platform.service.justice.gov.uk:${PORT}" member list
```

You will see list of members like this:

```
26153d5a331dc13d, started, etcd-a, https://etcd-a.internal.test-etcd-1.cloud-platform.service.justice.gov.uk:2380, https://etcd-a.internal.test-etcd-1.cloud-platform.service.justice.gov.uk:4001
27aaafc750f8d2f7, started, etcd-c, https://etcd-c.internal.test-etcd-1.cloud-platform.service.justice.gov.uk:2380, https://etcd-c.internal.test-etcd-1.cloud-platform.service.justice.gov.uk:4001
8d4a23328d324c94, started, etcd-b, https://etcd-b.internal.test-etcd-1.cloud-platform.service.justice.gov.uk:2380, https://etcd-b.internal.test-etcd-1.cloud-platform.service.justice.gov.uk:4001
```
The first value in each line is the ID of the member. We will need this to remove the member from the etcd cluster. 

Identify the unhealthy member (the old master node) and run the command below to remove the member.

Kops maintains a separate etcd cluster for events, so we need to remove the old node from the etcd-events cluster as well. Change the `PORT=4002` and `ETCD=etcd-events` and run it again to remove the old member from the events etcd cluster.

```bash
cd /opt/etcd-v3.3.10-linux-amd64/
CLUSTER=live-1
PORT=4001
ETCD=etcd
ETCDCTL_API=3 \
  ./etcdctl --key /rootfs/etc/kubernetes/pki/kube-apiserver/etcd-client.key \
  --cert  /rootfs/etc/kubernetes/pki/kube-apiserver/etcd-client.crt \
  --cacert /rootfs/etc/kubernetes/pki/kube-apiserver/etcd-ca.crt  \
  --endpoints "https://${ETCD}-a.internal.${CLUSTER}.cloud-platform.service.justice.gov.uk:${PORT},https://${ETCD}-b.internal.${CLUSTER}.cloud-platform.service.justice.gov.uk:${PORT},https://${ETCD}-c.internal.${CLUSTER}.cloud-platform.service.justice.gov.uk:${PORT}" member remove <member-id>
```

You will get a message:

```
Member 26153d5a331dc13d removed from cluster bdfd883e2bfa8594
```
Run the member list again and you will see the member has been removed:

```
27aaafc750f8d2f7, started, etcd-c, https://etcd-c.internal.test-etcd-1.cloud-platform.service.justice.gov.uk:2380, https://etcd-c.internal.test-etcd-1.cloud-platform.service.justice.gov.uk:4001
8d4a23328d324c94, started, etcd-b, https://etcd-b.internal.test-etcd-1.cloud-platform.service.justice.gov.uk:2380, https://etcd-b.internal.test-etcd-1.cloud-platform.service.justice.gov.uk:4001
```

##  Replacing the master via kops

You need to terminate the failed master and remove the volumes attached to it, and launch the new master with kops.

1) Back up volumes for both etcd main and events by creating snapshots from AWS console.

2) Take a etcdctl [backup][etcdctl-backup], by using `docker exec` to launch a shell on the `kopeio/etcd-manager` container in any of the working master nodes and run this command:

```bash
cd /opt/etcd-v3.3.10-linux-amd64/
CLUSTER=live-1
PORT=4001
ETCD=etcd
ETCDCTL_API=3 \
  ./etcdctl --key /rootfs/etc/kubernetes/pki/kube-apiserver/etcd-client.key \
  --cert  /rootfs/etc/kubernetes/pki/kube-apiserver/etcd-client.crt \
  --cacert /rootfs/etc/kubernetes/pki/kube-apiserver/etcd-ca.crt  \
  --endpoints "https://${ETCD}-a.internal.${CLUSTER}.cloud-platform.service.justice.gov.uk:${PORT},https://${ETCD}-b.internal.${CLUSTER}.cloud-platform.service.justice.gov.uk:${PORT},https://${ETCD}-c.internal.${CLUSTER}.cloud-platform.service.justice.gov.uk:${PORT}" snapshot save /rootfs/mnt/snapshot.db
```
You will get a message like:

```
Snapshot saved at /rootfs/mnt/snapshot.db
```

The snapshot.db is saved at `/rootfs/mnt/`, which is volume mounted to a local directory at `/mnt`. 

3) Edit the kops instance group for the master you removed, to scale down by replacing maxSize and minSize values to 0, and run a kops update on the cluster. e.g:

```
kops edit instancegroup master-eu-west-2a
```

```bash
kops update cluster ${CLUSTER_NAME}.cloud-platform.service.justice.gov.uk --yes
```

4) In the Amazon EC2 console, on the Instances page, search using `${CLUSTER_NAME}` to locate the failed master instance:
 
```
 master-<availability-zone>.masters.<cluster_name>.cloud-platform.service.justice.gov.uk
 ```
 
Terminate the failed master instance following this [guide][terminate-ec2-instance]. 

#### etcd volumes

Kubernetes live-1 cluster deployed with kops stores the etcd state in two different AWS EBS volumes per master node. One volume is used to store the Kubernetes main data, the other one for events. For a cluster with three masters this will result in six volumes for etcd data (one in each AZ).
 
5) In the Amazon EBS volume console, search using `${CLUSTER_NAME}` to locate the right volume associated with the broken master. Remove the volumes (main and events) for the failed master.

For example, for the master node instance 
 
```
master-eu-west-2a.masters.live-1.cloud-platform.service.justice.gov.uk
```

...you should see volumes called:
 
```
a.etcd-main.live-1.cloud-platform.service.justice.gov.uk
a.etcd-events.live-1.cloud-platform.service.justice.gov.uk
```

6) Create the new master instance with kops.

Edit the kops instancegroup, and scale back up by changing maxSize and minSize values to 1, then run a kops update on the cluster.

```
kops edit instancegroup master-eu-west-2a
```

```bash
kops update cluster ${CLUSTER_NAME}.cloud-platform.service.justice.gov.uk --yes
```

A new master will be launched, and should join the etcd cluster. This can take 10-15 minutes. After this, validate the cluster:

```bash
kops validate cluster ${CLUSTER_NAME}.cloud-platform.service.justice.gov.uk
```

You should see the new master in the list, and the cluster should be in `ready` status.

```
Your cluster ${CLUSTER_NAME}.cloud-platform.service.justice.gov.uk is ready
```

[remove-member-first]: https://github.com/etcd-io/etcd/blob/master/Documentation/faq.md#should-i-add-a-member-before-removing-an-unhealthy-member
[etcdctl-backup]: https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/recovery.md#snapshotting-the-keyspace
[terminate-ec2-instance]: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/terminating-instances.html
