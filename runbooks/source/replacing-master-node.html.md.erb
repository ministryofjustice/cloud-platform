---
title: Replacing a master node
weight: 225
---

# Replacing a master node

If one of the master nodes fails due to hardware failure, data directory corruption, etcd volume deletion, or some other problem, it should be replaced as soon as possible.

To replace the Master node, follow the instructions below for removing the unhealthy member from the cluster, and then re-adding the master node and its volumes via kops


## Pre-requisites

Before you begin, there are a few pre-reqs:

- You are targeting the right cluster.

```bash
$ kubectl config current-context
```

Expected outcome example: `live-1.cloud-platform.service.justice.gov.uk`

- You are using the right AWS_PROFILE for this cluster.

```bash
$ export AWS_PROFILE=moj-cp
```

- Export KOPS_STATE_STORE

```bash
$ export KOPS_STATE_STORE=s3://cloud-platform-kops-state
```

## Identify the failed master node

Run kops validate cluster to identify failed master node.

```bash
kops validate cluster ${CLUSTER_NAME}.cloud-platform.service.justice.gov.uk
```

You will see validation errors as below, which shows that one of the machines not joined the cluster. Make sure it is a master node by looking for the Instance ID, which should be named as `master-<availability-zone>.masters.<cluster_name>.cloud-platform.service.justice.gov.uk` in the Amazon EC2 console, on the Instances page.

 
```bash
VALIDATION ERRORS
KIND    NAME            MESSAGE
Machine i-046ed8d94d65719a6 machine "i-046ed8d94d65719a6" has not yet joined cluster
```

##  Remove the unhealthy master node.

As per [this guidance][remove-member-first] remove the unhealthy member first. Identify and remove the unhealthy member by accessing any of the working master nodes using SSH, via the bastion instance.

In the Amazon EC2 console, on the Instances page, search using `${CLUSTER_NAME}`to locate bastion instance (`bastion.<cluster_name>.cloud-platform.service.justice.gov.uk`). Use Public DNS (IPv4) In the description of the Instance to login in to bastion as below:


```bash
ssh -A admin@ec2-35-176-xx-xx.eu-west-2.compute.amazonaws.com -p 50422
```

From the bastion, login to any of the working master nodes:

```bash
ssh 172.20.xx.xx
```

While SSHed onto a master:

```bash
$ sudo docker ps | grep etcd.manager
```
You will see output as below:

```
e81b4622417b  kopeio/etcd-manager         k8s_etcd-manager_etcd-manager-main-...
39c186fba5ea  kopeio/etcd-manager         k8s_etcd-manager_etcd-manager-events-...
8c77d710ce46  k8s.gcr.io/pause-amd64:3.0  k8s_POD_etcd-manager-main-...
806e5af8125d  k8s.gcr.io/pause-amd64:3.0  k8s_POD_etcd-manager-events-...
```

Pick the kopeio/etcd-manager container you're interested in ('main' or 'events') and then docker exec onto it

```bash
$ sudo docker exec -it [container id] bash
```

Run etcdctl like this to get the member list of `etcd-main`. Change the `PORT=4002` and `ETCD=etcd-events` and run it again to get the member list of `etcd-events`

```bash
cd /opt/etcd-v3.3.10-linux-amd64/
CLUSTER=live-1
PORT=4001
ETCD=etcd
ETCDCTL_API=3 \
  ./etcdctl --key /rootfs/etc/kubernetes/pki/kube-apiserver/etcd-client.key \
  --cert  /rootfs/etc/kubernetes/pki/kube-apiserver/etcd-client.crt \
  --cacert /rootfs/etc/kubernetes/pki/kube-apiserver/etcd-ca.crt  \
  --endpoints "https://${ETCD}-a.internal.${CLUSTER}.cloud-platform.service.justice.gov.uk:${PORT},https://${ETCD}-b.internal.${CLUSTER}.cloud-platform.service.justice.gov.uk:${PORT},https://${ETCD}-c.internal.${CLUSTER}.cloud-platform.service.justice.gov.uk:${PORT}" member list
```

You will see list of members like this:

```
26153d5a331dc13d, started, etcd-a, https://etcd-a.internal.test-etcd-1.cloud-platform.service.justice.gov.uk:2380, https://etcd-a.internal.test-etcd-1.cloud-platform.service.justice.gov.uk:4001
27aaafc750f8d2f7, started, etcd-c, https://etcd-c.internal.test-etcd-1.cloud-platform.service.justice.gov.uk:2380, https://etcd-c.internal.test-etcd-1.cloud-platform.service.justice.gov.uk:4001
8d4a23328d324c94, started, etcd-b, https://etcd-b.internal.test-etcd-1.cloud-platform.service.justice.gov.uk:2380, https://etcd-b.internal.test-etcd-1.cloud-platform.service.justice.gov.uk:4001
```
The first value in each line is the ID of the member. We will need this to remove the member from the etcd cluster. 

Identify the unhealthy member (the old master node) and run the command below to remove the member.

Kops maintains a separate etcd cluster for events, so we need to remove the old node from the etcd-events cluster as well. Change the `PORT=4002` and `ETCD=etcd-events` and run it again to remove the old member from the events etcd cluster.

```bash
cd /opt/etcd-v3.3.10-linux-amd64/
CLUSTER=live-1
PORT=4001
ETCD=etcd
ETCDCTL_API=3 \
  ./etcdctl --key /rootfs/etc/kubernetes/pki/kube-apiserver/etcd-client.key \
  --cert  /rootfs/etc/kubernetes/pki/kube-apiserver/etcd-client.crt \
  --cacert /rootfs/etc/kubernetes/pki/kube-apiserver/etcd-ca.crt  \
  --endpoints "https://${ETCD}-a.internal.${CLUSTER}.cloud-platform.service.justice.gov.uk:${PORT},https://${ETCD}-b.internal.${CLUSTER}.cloud-platform.service.justice.gov.uk:${PORT},https://${ETCD}-c.internal.${CLUSTER}.cloud-platform.service.justice.gov.uk:${PORT}" member remove <member-id>
```

You will get a message:

```
Member 26153d5a331dc13d removed from cluster bdfd883e2bfa8594
```
Run the member list again and you will see the member has been removed:

```
27aaafc750f8d2f7, started, etcd-c, https://etcd-c.internal.test-etcd-1.cloud-platform.service.justice.gov.uk:2380, https://etcd-c.internal.test-etcd-1.cloud-platform.service.justice.gov.uk:4001
8d4a23328d324c94, started, etcd-b, https://etcd-b.internal.test-etcd-1.cloud-platform.service.justice.gov.uk:2380, https://etcd-b.internal.test-etcd-1.cloud-platform.service.justice.gov.uk:4001
```

##  Replacing the master via kops

To be sure that everything runs smoothly and is setup correctly, you need to terminate the broken master and remove the volumes attached to it, and launch the new master with kops.

1) Back up volumes for both etcd main and events by creating snapshots from AWS console.

2) Take a etcdctl [backup][etcdctl-backup], by docker exec onto kopeio/etcd-manager container in any of the working master nodes and run the below command.

```bash
cd /opt/etcd-v3.3.10-linux-amd64/
CLUSTER=live-1
PORT=4001
ETCD=etcd
ETCDCTL_API=3 \
  ./etcdctl --key /rootfs/etc/kubernetes/pki/kube-apiserver/etcd-client.key \
  --cert  /rootfs/etc/kubernetes/pki/kube-apiserver/etcd-client.crt \
  --cacert /rootfs/etc/kubernetes/pki/kube-apiserver/etcd-ca.crt  \
  --endpoints "https://${ETCD}-a.internal.${CLUSTER}.cloud-platform.service.justice.gov.uk:${PORT},https://${ETCD}-b.internal.${CLUSTER}.cloud-platform.service.justice.gov.uk:${PORT},https://${ETCD}-c.internal.${CLUSTER}.cloud-platform.service.justice.gov.uk:${PORT}" snapshot save /rootfs/mnt/snapshot.db
```
You will get a message as below. The snapshot.db is saved at `/rootfs/mnt/`, which is volume mounted to a local directory at `/mnt`. 

```
Snapshot saved at /rootfs/mnt/snapshot.db
```

3) Edit the kops instance group for the master you removed, to scale down by replacing maxSize and minSize values to 0, and run an kops update on the cluster.

```
kops edit instancegroup master-eu-west-2a
```

```bash
kops update cluster ${CLUSTER_NAME}.cloud-platform.service.justice.gov.uk --yes
```

4) In the Amazon EC2 console, on the Instances page, search using `${CLUSTER_NAME}` to locate broken master instance (`master-<availability-zone>.masters.<cluster_name>.cloud-platform.service.justice.gov.uk`). Terminate the broken master instance following this [guide][terminate-ec2-instance]. 

5) In the Amazon volume console, search using `${CLUSTER_NAME}` to locate the right volume associated with the broken master. Remove volumes (main and events) for the broken master.

For example, volumes will be named as below, for master `master-eu-west-2a.masters.live-1.cloud-platform.service.justice.gov.uk`.

```
a.etcd-main.live-1.cloud-platform.service.justice.gov.uk
a.etcd-events.live-1.cloud-platform.service.justice.gov.uk
```

6) Create the master instance with kops.

Edit the kops instancegroup, and scale back up by changing maxSize and minSize values to 1, then run a kops update on the cluster.

```
kops edit instancegroup master-eu-west-2a
```

```bash
kops update cluster ${CLUSTER_NAME}.cloud-platform.service.justice.gov.uk --yes
```

Master will be restarted with a clean config and should join the others. Give some time (10-15 min) and validate the cluster, the replacement master node should join the cluster and be in ready status.

```bash
kops validate cluster ${CLUSTER_NAME}.cloud-platform.service.justice.gov.uk
```

You can see the cluster is in ready status.

```
Your cluster ${CLUSTER_NAME}.cloud-platform.service.justice.gov.uk is ready
```
## etcd volumes

Kubernetes live-1 cluster deployed with kops stores the etcd state in two different AWS EBS volumes per master node. One volume is used to store the Kubernetes main data, the other one for events. For a cluster with three masters this will result in six volumes for etcd data (one in each AZ).



[remove-member-first]: https://github.com/etcd-io/etcd/blob/master/Documentation/faq.md#should-i-add-a-member-before-removing-an-unhealthy-member
[etcdctl-backup]: https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/recovery.md#snapshotting-the-keyspace
[terminate-ec2-instance]: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/terminating-instances.html